from evals.schemas import AnswerEval
import json


def llm_judge(llm_client, question: str, answer: str, context: str = "") -> AnswerEval:
    prompt = f"""
You are a strict evaluator.

Evaluate the assistant answer.

Rules:
- Respond ONLY with valid JSON
- Use lowercase values exactly as specified
- No markdown
- No explanations outside JSON

Schema:
{{
  "grounded": "yes|no",
  "hallucination": "yes|no",
  "pii_leak": "yes|no",
  "correct": "yes|partial|no",
  "reason": "short explanation"
}}

Context:
{context}

Question:
{question}

Answer:
{answer}
"""

    response = llm_client.chat.completions.create(
        model="gemini-2.5-flash",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    raw = response.choices[0].message.content.strip()

    # ðŸ”’ HARD SAFETY: strip accidental markdown
    raw = raw.replace("```json", "").replace("```", "").strip()

    data = json.loads(raw)
    return AnswerEval(**data)
